{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import backoff\n",
    "import time\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import shutil\n",
    "import os\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# For neural network stuff\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "import splitfolders\n",
    "\n",
    "\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def --> Create png folder only + encoderer link\n",
    "# -- dict = {'bodyid': material}\n",
    "# Then split the png to train and test\n",
    "\n",
    "def get_all_files(directory, pattern):\n",
    "    return [f for f in Path(directory).glob(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2name(id):\n",
    "    id = id.lower()\n",
    "    # id = id.rstrip()\n",
    "    if id == 'metal_non-ferrous':\n",
    "        return 'non-ferrous metal'\n",
    "    elif id == 'metal_ferrous':\n",
    "        return 'ferrous metal'\n",
    "    elif id == 'metal_ferrous_steel':\n",
    "        return 'steel'\n",
    "    elif id == 'metal_aluminum':\n",
    "        return 'aluminum'\n",
    "    elif id in ['other', 'wood', 'plastic']:\n",
    "        return id\n",
    "    else:\n",
    "        raise f\"Non-default id! {id}\"\n",
    "        # return 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r\"/Users/pat/Documents/Development/patbam/Fusion360GalleryDataset_23hackathon_train\"     # Change this to the directory where you downloaded the dataset\n",
    "input_jsons = get_all_files(input_dir, \"*/assembly.json\")\n",
    "\n",
    "# Create new folder for collecting png for training\n",
    "output_dir = \"/Users/pat/Documents/Development/patbam/PNG_NeuralNetwork\"\n",
    "\n",
    "# Classify the png folder in to material class\n",
    "# print((input_jsons[0]))\n",
    "\n",
    "# Create data frame for body part\n",
    "df_bodyPart = pd.DataFrame(columns = ['name', 'area', 'volume','material'])\n",
    "\n",
    "col = ['name', 'area', 'volume','material']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6336/6336 [00:14<00:00, 426.86it/s]\n"
     ]
    }
   ],
   "source": [
    "assemblies = {}\n",
    "counter = 0\n",
    "desired_img_sz = (224, 224)  # Desired size of the image\n",
    "\n",
    "for input_json in tqdm(input_jsons): # tqdm: to show the progress bar \n",
    "\n",
    "    with open(input_json, \"r\", encoding=\"utf-8\") as f:\n",
    "        assembly_data = json.load(f)\n",
    "\n",
    "    bodies = []\n",
    "    # print(input_json)\n",
    "\n",
    "    # ------ get name, material, area, volume ----------------\n",
    "    for key, value in assembly_data['bodies'].items():\n",
    "        name = value['name']\n",
    "        material = value['material_category']\n",
    "        area = value['physical_properties']['area']\n",
    "        volume = value['physical_properties']['volume']\n",
    "        png_name = value['png']\n",
    "\n",
    "        # print(name, \" \", png_name)\n",
    "\n",
    "\n",
    "        if name[:4] != 'Body':  # some of the bodies do not have a custom name. This filters those bodies out.\n",
    "            \n",
    "            # bodies.append({'name':name, 'material':material})\n",
    "            bodies.append({'id': key, 'name':name, 'material': material, 'area': area, 'volume': volume})\n",
    "\n",
    "            # Will include the data with num_joint later\n",
    "\n",
    "            # Add data to pandas             \n",
    "            df_curBody = {'name':name, 'area': area, 'volume':volume, 'material':material}\n",
    "            df_bodyPart.loc[len(df_bodyPart)] = df_curBody # Add the body part data to dataFrame\n",
    "\n",
    "\n",
    "             # ----------For categorizing the pictures to material base ----------------------\n",
    "            if material == \"Metal_Aluminum\":\n",
    "                destination_path = output_dir + \"/metal_aluminum/\" +  png_name\n",
    "            elif material == \"Metal_Ferrous_Steel\":\n",
    "                destination_path = output_dir + \"/metal_ferrous_steel/\" +  png_name\n",
    "            elif material == \"Metal_Non-Ferrous\":\n",
    "                destination_path = output_dir + \"/metal_non-ferrous/\" +  png_name\n",
    "            elif material == \"Metal_Ferrous\":\n",
    "                destination_path = output_dir + \"/metal_ferrous/\" +  png_name\n",
    "            elif material == \"Wood\":\n",
    "                destination_path = output_dir + \"/wood/\" +  png_name       \n",
    "            elif material == \"Plastic\":\n",
    "                destination_path = output_dir + \"/plastic/\" +  png_name\n",
    "            else:\n",
    "                destination_path = output_dir + \"/other/\" +  png_name\n",
    "\n",
    "            # print(destination_path)\n",
    "            # ---- copy png file with resized to PNG_NeuralNetwork ----\n",
    "            # Get the source path\n",
    "            # cur_json = str(input_json)\n",
    "            # cur_folder = cur_json.replace(\"assembly.json\", \"\")\n",
    "\n",
    "\n",
    "            # Remove \"assembly.json\" by getting the parent directories\n",
    "            cur_folder = input_json.parents[0] / input_json.name.replace(\"assembly.json\", \"\")\n",
    "            # print(cur_folder)\n",
    "\n",
    "\n",
    "            source_path = os.path.join(cur_folder, png_name)\n",
    "\n",
    "\n",
    "            # Open the image using PIL\n",
    "            image = Image.open(source_path)\n",
    "\n",
    "            # Resize the image while maintaining aspect ratio\n",
    "            image.thumbnail(desired_img_sz, Image.ANTIALIAS)\n",
    "\n",
    "            #destination_path = os.path.join(output_dir, png_name)\n",
    "            \n",
    "\n",
    "\n",
    "            #shutil.copy(source_path, destination_path) # this is for the copy\n",
    "            image.save(destination_path)\n",
    "\n",
    "            # ----------------------------------------------------------------------- #\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "    if len(bodies) > 0:\n",
    "        assemblies[input_json.parts[-2]] = bodies\n",
    "    counter +=1 \n",
    "\n",
    "# assemblies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_bodyPart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete data in ./output folder (in case not run it for the 1st time)\n",
    "item_path = \"./output\"\n",
    "shutil.rmtree(item_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 13885 files [00:04, 2798.10 files/s]\n"
     ]
    }
   ],
   "source": [
    "# Train-validation-test\n",
    "splitfolders.ratio(\"./\", output=\"output\", seed=1337, ratio=(.7, .15, .15), group_prefix=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_names_list(folder_path): # Get file name from the folder path that have material subfolder\n",
    "    file_names = []\n",
    "    material_names = ['metal_aluminum', 'metal_ferrous_steel','metal_ferrous','metal_non-ferrous','wood','other','plastic']\n",
    "    for material in material_names:\n",
    "        cur_folder = os.path.join(folder_path, material)\n",
    "        files_path = get_all_files(cur_folder,\"*\")\n",
    "        for file_path in files_path:\n",
    "            file_name = os.path.basename(file_path)\n",
    "            file_name = os.path.splitext(file_name)[0]\n",
    "            file_names.append(file_name)\n",
    "    return file_names\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"./output/train/\"     # current location of train directory\n",
    "# List all files in the directory\n",
    "file_names_train = get_file_names_list(train_dir)\n",
    "\n",
    "\n",
    "val_dir = r\"./output/val/\"     # current location of validation directory\n",
    "# List all files in the directory\n",
    "file_names_val = get_file_names_list(val_dir)\n",
    "\n",
    "\n",
    "test_dir = r\"./output/test/\"     # current location of test directory\n",
    "# List all files in the directory\n",
    "file_names_test = get_file_names_list(test_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_DIMS = (224, 224, 3)\n",
    "train_data_dir = './output/train/'\n",
    "validation_data_dir = './output/val/'\n",
    "batch_size=36\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9716 images belonging to 7 classes.\n",
      "Found 2078 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_data_dir,\n",
    "        target_size=(IMAGE_DIMS[0], IMAGE_DIMS[1]),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_data_dir,\n",
    "        target_size=(IMAGE_DIMS[0], IMAGE_DIMS[1]),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_train_samples =9716\n",
    "nb_validation_samples =2078"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cnn model\n",
    "def define_model(h,w):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "    padding='same', input_shape=(h,w, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "    padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "    padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_diagnostics(history):\n",
    "# plot loss\n",
    "    epochs = 8\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    N = epochs\n",
    "    plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "    plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "    plt.title(\"Loss\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    # plot accuracy\n",
    "    plt.style.use(\"ggplot\")\n",
    "    plt.figure()\n",
    "    N = epochs\n",
    "    plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"accuracy\")\n",
    "    plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.xlabel(\"Epoch #\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    # save plot to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/c47jx6qs4_9f998yg8lg5mq00000gp/T/ipykernel_44803/1749340754.py:7: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - ETA: 0s - loss: 4.2822 - accuracy: 0.2306\n",
      "Epoch 1: loss improved from inf to 4.28223, saving model to ./weights.h5\n",
      "308/308 [==============================] - 219s 708ms/step - loss: 4.2822 - accuracy: 0.2306 - val_loss: 1.8181 - val_accuracy: 0.2291\n",
      "Epoch 2/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - ETA: 0s - loss: 1.7764 - accuracy: 0.2625\n",
      "Epoch 2: loss improved from 4.28223 to 1.77636, saving model to ./weights.h5\n",
      "308/308 [==============================] - 228s 741ms/step - loss: 1.7764 - accuracy: 0.2625 - val_loss: 1.7365 - val_accuracy: 0.3167\n",
      "Epoch 3/8\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.6713 - accuracy: 0.3450\n",
      "Epoch 3: loss improved from 1.77636 to 1.67130, saving model to ./weights.h5\n",
      "308/308 [==============================] - 234s 759ms/step - loss: 1.6713 - accuracy: 0.3450 - val_loss: 1.6548 - val_accuracy: 0.3604\n",
      "Epoch 4/8\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.5781 - accuracy: 0.3873\n",
      "Epoch 4: loss improved from 1.67130 to 1.57808, saving model to ./weights.h5\n",
      "308/308 [==============================] - 241s 781ms/step - loss: 1.5781 - accuracy: 0.3873 - val_loss: 1.5993 - val_accuracy: 0.3939\n",
      "Epoch 5/8\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.4784 - accuracy: 0.4389\n",
      "Epoch 5: loss improved from 1.57808 to 1.47839, saving model to ./weights.h5\n",
      "308/308 [==============================] - 232s 752ms/step - loss: 1.4784 - accuracy: 0.4389 - val_loss: 1.5843 - val_accuracy: 0.4080\n",
      "Epoch 6/8\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.3740 - accuracy: 0.4846\n",
      "Epoch 6: loss improved from 1.47839 to 1.37402, saving model to ./weights.h5\n",
      "308/308 [==============================] - 236s 765ms/step - loss: 1.3740 - accuracy: 0.4846 - val_loss: 1.5633 - val_accuracy: 0.4315\n",
      "Epoch 7/8\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.2567 - accuracy: 0.5384\n",
      "Epoch 7: loss improved from 1.37402 to 1.25673, saving model to ./weights.h5\n",
      "308/308 [==============================] - 235s 764ms/step - loss: 1.2567 - accuracy: 0.5384 - val_loss: 1.4966 - val_accuracy: 0.4812\n",
      "Epoch 8/8\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.1348 - accuracy: 0.5855\n",
      "Epoch 8: loss improved from 1.25673 to 1.13476, saving model to ./weights.h5\n",
      "308/308 [==============================] - 250s 811ms/step - loss: 1.1348 - accuracy: 0.5855 - val_loss: 1.5202 - val_accuracy: 0.4949\n"
     ]
    }
   ],
   "source": [
    "# Model 1: default setting from \n",
    "# URL: https://www.kaggle.com/code/koheimuramatsu/model-explainability-in-industrial-image-detection\n",
    "\n",
    "model = define_model(IMAGE_DIMS[0],IMAGE_DIMS[1])\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "epochs = 8\n",
    "batch_size = 36\n",
    "checkpoint = ModelCheckpoint(\"./weights.h5\",monitor=\"loss\",mode=\"min\",save_best_only = True,verbose=1)\n",
    "callbacks = [checkpoint]\n",
    "history = model.fit_generator(train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/18/c47jx6qs4_9f998yg8lg5mq00000gp/T/ipykernel_44803/3388313187.py:30: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - ETA: 0s - loss: 4.6918 - accuracy: 0.2440\n",
      "Epoch 1: loss improved from inf to 4.69176, saving model to ./weights.h5\n",
      "308/308 [==============================] - 410s 1s/step - loss: 4.6918 - accuracy: 0.2440 - val_loss: 1.7611 - val_accuracy: 0.2994\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308/308 [==============================] - ETA: 0s - loss: 1.7200 - accuracy: 0.3128\n",
      "Epoch 2: loss improved from 4.69176 to 1.71997, saving model to ./weights.h5\n",
      "308/308 [==============================] - 418s 1s/step - loss: 1.7200 - accuracy: 0.3128 - val_loss: 1.7013 - val_accuracy: 0.3384\n",
      "Epoch 3/10\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.6140 - accuracy: 0.3673\n",
      "Epoch 3: loss improved from 1.71997 to 1.61400, saving model to ./weights.h5\n",
      "308/308 [==============================] - 414s 1s/step - loss: 1.6140 - accuracy: 0.3673 - val_loss: 1.6192 - val_accuracy: 0.3810\n",
      "Epoch 4/10\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.5194 - accuracy: 0.4156\n",
      "Epoch 4: loss improved from 1.61400 to 1.51939, saving model to ./weights.h5\n",
      "308/308 [==============================] - 420s 1s/step - loss: 1.5194 - accuracy: 0.4156 - val_loss: 1.5679 - val_accuracy: 0.4167\n",
      "Epoch 5/10\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.4375 - accuracy: 0.4506\n",
      "Epoch 5: loss improved from 1.51939 to 1.43745, saving model to ./weights.h5\n",
      "308/308 [==============================] - 408s 1s/step - loss: 1.4375 - accuracy: 0.4506 - val_loss: 1.5196 - val_accuracy: 0.4322\n",
      "Epoch 6/10\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.3469 - accuracy: 0.4893\n",
      "Epoch 6: loss improved from 1.43745 to 1.34691, saving model to ./weights.h5\n",
      "308/308 [==============================] - 438s 1s/step - loss: 1.3469 - accuracy: 0.4893 - val_loss: 1.4835 - val_accuracy: 0.4495\n",
      "Epoch 7/10\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.2576 - accuracy: 0.5294\n",
      "Epoch 7: loss improved from 1.34691 to 1.25758, saving model to ./weights.h5\n",
      "308/308 [==============================] - 425s 1s/step - loss: 1.2576 - accuracy: 0.5294 - val_loss: 1.4646 - val_accuracy: 0.4794\n",
      "Epoch 8/10\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.1523 - accuracy: 0.5698\n",
      "Epoch 8: loss improved from 1.25758 to 1.15233, saving model to ./weights.h5\n",
      "308/308 [==============================] - 424s 1s/step - loss: 1.1523 - accuracy: 0.5698 - val_loss: 1.4426 - val_accuracy: 0.4870\n",
      "Epoch 9/10\n",
      "308/308 [==============================] - ETA: 0s - loss: 1.0633 - accuracy: 0.6083\n",
      "Epoch 9: loss improved from 1.15233 to 1.06327, saving model to ./weights.h5\n",
      "308/308 [==============================] - 423s 1s/step - loss: 1.0633 - accuracy: 0.6083 - val_loss: 1.4310 - val_accuracy: 0.5014\n",
      "Epoch 10/10\n",
      "308/308 [==============================] - ETA: 0s - loss: 0.9826 - accuracy: 0.6384\n",
      "Epoch 10: loss improved from 1.06327 to 0.98259, saving model to ./weights.h5\n",
      "308/308 [==============================] - 392s 1s/step - loss: 0.9826 - accuracy: 0.6384 - val_loss: 1.4208 - val_accuracy: 0.5141\n"
     ]
    }
   ],
   "source": [
    "# Model 1.2: start training LLM\n",
    "def define_model2(h,w):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (7, 7), activation='relu', kernel_initializer='he_uniform',\n",
    "    padding='same', input_shape=(h,w, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "    padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform',\n",
    "    padding='same'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(7, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "model = define_model2(IMAGE_DIMS[0],IMAGE_DIMS[1])\n",
    "model.compile(loss = 'categorical_crossentropy',optimizer = 'adam',metrics = ['accuracy'])\n",
    "epochs = 10\n",
    "batch_size = 36\n",
    "checkpoint = ModelCheckpoint(\"./weights.h5\",monitor=\"loss\",mode=\"min\",save_best_only = True,verbose=1)\n",
    "callbacks = [checkpoint]\n",
    "history = model.fit_generator(train_generator,\n",
    "    steps_per_epoch = nb_train_samples // batch_size,\n",
    "    epochs = epochs,\n",
    "    callbacks = callbacks,\n",
    "    validation_data = validation_generator,\n",
    "    validation_steps = nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Split the image to train and test set \n",
    "\n",
    "# body_list = [assembly_data for assembly_data in assemblies.values()]\n",
    "\n",
    "# train_set, test_set = train_test_split(body_list, test_size=0.2, shuffle=True, random_state=0)\n",
    "\n",
    "# cur_png_folder = output_dir\n",
    "# train_set_dir = \"/Users/pat/Documents/Development/patbam/PNG_NeuralNetwork/train_set\"\n",
    "# test_set_dir = \"/Users/pat/Documents/Development/patbam/PNG_NeuralNetwork/test_set\"\n",
    "\n",
    "\n",
    "# # Train set\n",
    "\n",
    "# for train_assem in train_set:\n",
    "#     for train_body in train_assem:\n",
    "#         png_name = train_body['id'] + '.png'\n",
    "#         source_path = os.path.join(cur_png_folder, png_name)\n",
    "#         destination_path = os.path.join(train_set_dir, png_name)\n",
    "#         shutil.move(source_path, destination_path)\n",
    "\n",
    "# # Test set\n",
    "\n",
    "# for test_assem in test_set:\n",
    "#     for test_body in test_assem:\n",
    "#         png_name = test_body['id'] + '.png'\n",
    "#         source_path = os.path.join(cur_png_folder, png_name)\n",
    "#         destination_path = os.path.join(test_set_dir, png_name)\n",
    "#         shutil.move(source_path, destination_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the CNN architecture\n",
    "# model = keras.Sequential([\n",
    "#     layers.Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "#     layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "#     layers.MaxPooling2D((2, 2)),\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(128, activation='relu'),\n",
    "#     layers.Dense(7, activation='softmax')  # Assuming 10 classes\n",
    "# ])\n",
    "\n",
    "# # Compile the model\n",
    "# model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# # Load and preprocess PNG images using ImageDataGenerator\n",
    "\n",
    "# image_size = (224, 224)\n",
    "# batch_size = 36\n",
    "\n",
    "# train_datagen = ImageDataGenerator(rescale=1.0/255.0, validation_split=0.2)\n",
    "\n",
    "\n",
    "# # ###########\n",
    "# # # List all files in the folder\n",
    "# # file_list = os.listdir('/Users/pat/Documents/Development/patbam/PNG_NeuralNetwork/train_set/')\n",
    "\n",
    "\n",
    "# # # Count the number of files\n",
    "# # num_files = len(file_list)\n",
    "\n",
    "# # print(\"Number of files in the folder:\", num_files)\n",
    "\n",
    "# # #############\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#     './train_set/',\n",
    "#     target_size=image_size,\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "#     subset='training'\n",
    "# )\n",
    "\n",
    "# validation_generator = train_datagen.flow_from_directory(\n",
    "#     './PNG_NeuralNetwork/train_set/',\n",
    "#     target_size=image_size,\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical',\n",
    "#     subset='validation'\n",
    "# )\n",
    "\n",
    "\n",
    "# # Get a batch of images and labels\n",
    "# batch_images, batch_labels = next(validation_generator)\n",
    "\n",
    "# # Print the shape of the batch of images\n",
    "# print(\"Batch of images shape:\", batch_images.shape)\n",
    "\n",
    "\n",
    "# try:\n",
    "#     batch_images, batch_labels = next(validation_generator)\n",
    "#     print(\"Images found and loaded successfully.\")\n",
    "# except Exception as e:\n",
    "#     print(\"An error occurred while loading images:\", e)\n",
    "\n",
    "# # Train the model\n",
    "# epochs = 10\n",
    "# history = model.fit(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "\n",
    "# # Evaluate the model\n",
    "# test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "# test_generator = test_datagen.flow_from_directory(\n",
    "#     './PNG_NeuralNetwork/test_set/',\n",
    "#     target_size=image_size,\n",
    "#     batch_size=batch_size,\n",
    "#     class_mode='categorical'\n",
    "# )\n",
    "# test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "\n",
    "# print(f'Test accuracy: {test_accuracy}')\n",
    "\n",
    "\n",
    "\n",
    "# # ------------ THIS SECTION IS IN FORMAT DEBATE BETWEEN OG/COMPRESS FILESIZE IN IMAGE PREPARATION ---------------\n",
    "            #  ------- Only copy with OG size\n",
    "\n",
    "            # # Get the source path\n",
    "            # cur_json = str(input_json)\n",
    "            # cur_folder = cur_json.replace(\"assembly.json\", \"\")\n",
    "            # source_path = os.path.join(cur_folder, png_name)\n",
    "\n",
    "            # destination_path = os.path.join(output_dir, png_name)\n",
    "            \n",
    "            # shutil.copy(source_path, destination_path) # this is for the copy\n",
    "# # ----------------------------------------------------------------------------------------------------------------\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
